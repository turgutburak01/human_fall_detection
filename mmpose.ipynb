{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WO6qff2EObm",
        "outputId": "41e5af0c-8d69-4891-c78a-aa9803b67458"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jun  7 17:43:20 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   77C    P0    32W /  70W |   2653MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xDOoC2aESaG",
        "outputId": "0f3af96c-a5ab-4d1a-9c78-4518bb4e088d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/gdrive/MyDrive/test_videos.zip /content/gdrive/MyDrive/test_videos"
      ],
      "metadata": {
        "id": "SPTenT-h9VVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcZ7FMbyEbZM"
      },
      "outputs": [],
      "source": [
        "!nvcc -V\n",
        "\n",
        "!gcc --version\n",
        "\n",
        "!which python\n",
        "\n",
        "%pip install torch==1.10.0+cu111 torchvision==0.11.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "%pip install -U openmim\n",
        "!mim install mmengine\n",
        "!mim install \"mmcv>=2.0.0rc1\"\n",
        "!mim install \"mmdet>=3.0.0rc0\"\n",
        "\n",
        "!git clone https://github.com/open-mmlab/mmpose.git\n",
        "\n",
        "%cd mmpose\n",
        "\n",
        "%pip install -v -e .\n",
        "\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39VNAxUyFV2B",
        "outputId": "02aecc77-e450-4c35-8d32-e1baa051d532"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version: 2.0.1+cu118 True\n",
            "torchvision version: 0.15.2+cu118\n",
            "mmpose version: 1.0.0\n",
            "cuda version: 11.8\n",
            "compiler information: GCC 9.3\n"
          ]
        }
      ],
      "source": [
        "# Check Pytorch installation\n",
        "import torch, torchvision\n",
        "\n",
        "print('torch version:', torch.__version__, torch.cuda.is_available())\n",
        "print('torchvision version:', torchvision.__version__)\n",
        "\n",
        "# Check MMPose installation\n",
        "import mmpose\n",
        "\n",
        "print('mmpose version:', mmpose.__version__)\n",
        "\n",
        "# Check mmcv installation\n",
        "from mmcv.ops import get_compiling_cuda_version, get_compiler_version\n",
        "\n",
        "print('cuda version:', get_compiling_cuda_version())\n",
        "print('compiler information:', get_compiler_version())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJX5Fdl_JKcw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "9adbb084-8562-4909-9700-f37f9f960204"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.22.4)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.3 which is incompatible.\n",
            "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install --upgrade numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com/7w7fccy7ky-4.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRFRXZctFB-G",
        "outputId": "e0be15b6-c020-4da9-a08a-57a8927de927"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-06 15:15:36--  https://prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com/7w7fccy7ky-4.zip\n",
            "Resolving prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com (prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com)... 52.218.89.152, 3.5.65.1, 52.92.20.210, ...\n",
            "Connecting to prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com (prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com)|52.218.89.152|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8326349365 (7.8G) [application/zip]\n",
            "Saving to: ‘7w7fccy7ky-4.zip’\n",
            "\n",
            "7w7fccy7ky-4.zip    100%[===================>]   7.75G  47.0MB/s    in 2m 42s  \n",
            "\n",
            "2023-06-06 15:18:18 (49.2 MB/s) - ‘7w7fccy7ky-4.zip’ saved [8326349365/8326349365]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip 7w7fccy7ky-4.zip"
      ],
      "metadata": {
        "id": "wNngX3XJGecx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXyuRbkBFV4w",
        "outputId": "e794f9e1-dc4b-4c51-c00a-0065d77b0ef9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/mmpose\n",
            "Loads checkpoint by http backend from path: https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth\" to /root/.cache/torch/hub/checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loads checkpoint by http backend from path: https://download.openmmlab.com/mmpose/v1/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w48_udp-8xb32-210e_coco-384x288-70d7ab01_20220913.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.openmmlab.com/mmpose/v1/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w48_udp-8xb32-210e_coco-384x288-70d7ab01_20220913.pth\" to /root/.cache/torch/hub/checkpoints/td-hm_hrnet-w48_udp-8xb32-210e_coco-384x288-70d7ab01_20220913.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model and loaded state dict do not match exactly\n",
            "\n",
            "unexpected key in source state_dict: data_preprocessor.mean, data_preprocessor.std\n",
            "\n",
            "06/07 15:32:00 - mmengine - WARNING - `Visualizer` backend is not initialized because save_dir is None.\n"
          ]
        }
      ],
      "source": [
        "%cd /content/mmpose/\n",
        "\n",
        "import mmcv\n",
        "from mmcv import imread\n",
        "import mmengine\n",
        "from mmengine.registry import init_default_scope\n",
        "import numpy as np\n",
        "\n",
        "from mmpose.apis import inference_topdown\n",
        "from mmpose.apis import init_model as init_pose_estimator\n",
        "from mmpose.evaluation.functional import nms\n",
        "from mmpose.registry import VISUALIZERS\n",
        "from mmpose.structures import merge_data_samples\n",
        "\n",
        "try:\n",
        "    from mmdet.apis import inference_detector, init_detector\n",
        "    has_mmdet = True\n",
        "except (ImportError, ModuleNotFoundError):\n",
        "    has_mmdet = False\n",
        "\n",
        "local_runtime = False\n",
        "\n",
        "try:\n",
        "    from google.colab.patches import cv2_imshow  # Colab için resim görselleştirme\n",
        "except:\n",
        "    local_runtime = True\n",
        "\n",
        "pose_config = 'configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w48_udp-8xb32-210e_coco-384x288.py'\n",
        "pose_checkpoint = 'https://download.openmmlab.com/mmpose/v1/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w48_udp-8xb32-210e_coco-384x288-70d7ab01_20220913.pth'\n",
        "det_config = 'demo/mmdetection_cfg/faster_rcnn_r50_fpn_coco.py'\n",
        "det_checkpoint = 'https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth'\n",
        "\n",
        "device = 'cuda:0'\n",
        "cfg_options = dict(model=dict(test_cfg=dict(output_heatmaps=False)))\n",
        "\n",
        "\n",
        "# build detector\n",
        "detector = init_detector(\n",
        "    det_config,\n",
        "    det_checkpoint,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# build pose estimator\n",
        "pose_estimator = init_pose_estimator(\n",
        "    pose_config,\n",
        "    pose_checkpoint,\n",
        "    device=device,\n",
        "    cfg_options=cfg_options\n",
        ")\n",
        "\n",
        "# init visualizer\n",
        "pose_estimator.cfg.visualizer.radius = 3\n",
        "pose_estimator.cfg.visualizer.line_width = 1\n",
        "visualizer = VISUALIZERS.build(pose_estimator.cfg.visualizer)\n",
        "\n",
        "visualizer.set_dataset_meta(pose_estimator.dataset_meta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cen3pN4tI_Kk"
      },
      "outputs": [],
      "source": [
        "\n",
        "def visualize_img(img_path, detector, pose_estimator, visualizer,\n",
        "                  show_interval, out_file):\n",
        "\n",
        "    # predict bbox\n",
        "    init_default_scope(detector.cfg.get('default_scope', 'mmdet'))\n",
        "    detect_result = inference_detector(detector, img_path)\n",
        "    pred_instance = detect_result.pred_instances.cpu().numpy()\n",
        "    bboxes = np.concatenate(\n",
        "        (pred_instance.bboxes, pred_instance.scores[:, None]), axis=1)\n",
        "    bboxes = bboxes[np.logical_and(pred_instance.labels == 0,\n",
        "                                   pred_instance.scores > 0.3)]\n",
        "    bboxes = bboxes[nms(bboxes, 0.3)][:, :4]\n",
        "\n",
        "    # predict keypoints\n",
        "    pose_results = inference_topdown(pose_estimator, img_path, bboxes)\n",
        "    data_samples = merge_data_samples(pose_results)\n",
        "\n",
        "    # show the results\n",
        "    img = mmcv.imread(img_path)\n",
        "    img = mmcv.imconvert(img, 'bgr', 'rgb')\n",
        "\n",
        "    image = np.zeros((480, 640, 3), dtype=np.uint8)\n",
        "    image.fill(255)\n",
        "\n",
        "    visualizer.add_datasample(\n",
        "        'result',\n",
        "        image,\n",
        "        data_sample=data_samples,\n",
        "        draw_gt=False,\n",
        "        draw_heatmap=False,\n",
        "        draw_bbox=False,\n",
        "        show=False,\n",
        "        wait_time=show_interval,\n",
        "        out_file=out_file,\n",
        "        kpt_thr=0.3)\n",
        "\n",
        "    return pose_results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img = '/content/cas400134.png'\n",
        "visualize_img(\n",
        "    img,\n",
        "    detector,\n",
        "    pose_estimator,\n",
        "    visualizer,\n",
        "    show_interval=0,\n",
        "    out_file=None)\n",
        "\n",
        "vis_result = visualizer.get_image();\n",
        "\n",
        "cv2_imshow(vis_result[:,:,::-1]);"
      ],
      "metadata": {
        "id": "BZ1504CCrL9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "\n",
        "import os\n",
        "\n",
        "folder_names = ['Fall backwards', 'Fall forward', 'Fall left', 'Fall right', 'Fall sitting', 'Hop', 'Kneel', 'Pick up object', 'Sit down', 'Walk']\n",
        "parent_folder = 'dataset'\n",
        "\n",
        "for i in range(1, 11):\n",
        "  os.makedirs(f\"dataset/Subject.{i}\", exist_ok=True)\n",
        "  subject_folder = f'Subject.{i}'\n",
        "  for folder_name in folder_names:\n",
        "    folder_path = os.path.join(parent_folder, subject_folder, folder_name)\n",
        "    os.makedirs(folder_path, exist_ok=True)"
      ],
      "metadata": {
        "id": "rG_xhBz0OvRN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "764777c8-5793-4afd-8b50-a0a01d8ad8cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "main_parent_folder = 'CAUCAFall'\n",
        "\n",
        "for i in range(1, 11):\n",
        "  subject_folder = f'Subject.{i}'\n",
        "  subject_path = os.path.join(main_parent_folder, subject_folder)\n",
        "\n",
        "  for folder_name in folder_names:\n",
        "    folder_path = os.path.join(subject_path, folder_name)\n",
        "    file_list = os.listdir(folder_path)\n",
        "    png_list = [file_name for file_name in file_list if file_name.endswith('.png')]\n",
        "\n",
        "    for png_file in png_list:\n",
        "      png_path = os.path.join(folder_path, png_file)\n",
        "      out_file = os.path.join('/content', parent_folder, subject_folder, folder_name, png_file)\n",
        "      results = visualize_img(png_path, detector, pose_estimator, visualizer, show_interval=0, out_file=out_file)"
      ],
      "metadata": {
        "id": "bgnPDqNprxe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import models, transforms, datasets\n",
        "from PIL import Image\n",
        "\n",
        "model = models.vgg16(pretrained=True)\n",
        "\n",
        "print(\"Önceki Classifier Hali:\")\n",
        "print(model.classifier)\n",
        "\n",
        "# Modelin sınıflandırma katmanlarını alın\n",
        "classifier = list(model.classifier.children())\n",
        "\n",
        "# Son iki tam bağlantılı katmanı kaldırın\n",
        "classifier = classifier[:-3]\n",
        "\n",
        "# Yeni sınıflandırma katmanını oluşturun\n",
        "new_classifier = nn.Sequential(*classifier)\n",
        "\n",
        "# Modelin sınıflandırma katmanını güncelleyin\n",
        "model.classifier = new_classifier\n",
        "\n",
        "# Son sınıflandırma katmanından sonra gelen parametreleri dondurucu yapın\n",
        "for param in model.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "print(\"Sonraki Classifier Hali:\")\n",
        "print(model.classifier)\n",
        "\n",
        "# Modeli GPU'ya taşıma\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLdV1LxG6x5H",
        "outputId": "20c3a344-5a45-4802-ff8a-3ca0401f1b34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:02<00:00, 275MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Önceki Classifier Hali:\n",
            "Sequential(\n",
            "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "  (1): ReLU(inplace=True)\n",
            "  (2): Dropout(p=0.5, inplace=False)\n",
            "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "  (4): ReLU(inplace=True)\n",
            "  (5): Dropout(p=0.5, inplace=False)\n",
            "  (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
            ")\n",
            "Sonraki Classifier Hali:\n",
            "Sequential(\n",
            "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "  (1): ReLU(inplace=True)\n",
            "  (2): Dropout(p=0.5, inplace=False)\n",
            "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('/content/gdrive/MyDrive/vgg16_pose_model2.pth'))\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRP2jdMX6rjN",
        "outputId": "f2134c21-4d52-4d1e-bfda-c7d48401d7f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): ReLU(inplace=True)\n",
            "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): ReLU(inplace=True)\n",
            "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img = \"/content/cis700211.png\""
      ],
      "metadata": {
        "id": "szaHVQ025J42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeCI9AAdLIpZ"
      },
      "outputs": [],
      "source": [
        "results= visualize_img(\n",
        "    img,\n",
        "    detector,\n",
        "    pose_estimator,\n",
        "    visualizer,\n",
        "    show_interval=0,\n",
        "    out_file=None)\n",
        "\n",
        "vis_result = visualizer.get_image()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image = Image.fromarray(vis_result)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "input_image = transform(image).unsqueeze(0)  # Görüntüyü tensör formuna dönüştürme ve boyutunu genişletme\n"
      ],
      "metadata": {
        "id": "7ByI9opX7jfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Video dosyasını açın\n",
        "video_path = \"/content/gdrive/MyDrive/test_videos/subject8/WalkS8.avi\"\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Video özelliklerini alın\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "# Video kaydedici\n",
        "output_path = \"/content/gdrive/MyDrive/test_videos/subject8/WalkS8_test.mp4\"\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "# Font ayarları\n",
        "font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "font_scale = 1\n",
        "font_thickness = 2\n",
        "text_position = (int(frame_width/2), 30)\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Frame'i pose tahmini ve görselleştirme için gönderme\n",
        "    pose_results = visualize_img(frame, detector, pose_estimator, visualizer, show_interval=0, out_file=None)\n",
        "\n",
        "    # Görselleştirme sonucunu alın\n",
        "    vis_result = visualizer.get_image()\n",
        "\n",
        "    # Görüntüyü PIL formatına dönüştürme\n",
        "    pil_image = Image.fromarray(vis_result)\n",
        "\n",
        "    # Görüntüyü dönüştürme\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    input_image = transform(pil_image).unsqueeze(0)\n",
        "\n",
        "    # Görüntüyü CUDA cihazına gönderme\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    input_image = input_image.to(device)\n",
        "\n",
        "    class_names = ['fall', 'non-fall']\n",
        "\n",
        "    # Tahmin yapma\n",
        "    output = model(input_image)\n",
        "    _, predicted = torch.max(output, 1)\n",
        "    print(class_names[predicted.item()])\n",
        "\n",
        "    # Tahmin sonucunu görüntüye yazdırma\n",
        "    predicted_class = class_names[predicted.item()]\n",
        "    cv2.putText(frame, predicted_class, text_position, font, font_scale, (0, 255, 0), font_thickness)\n",
        "\n",
        "    # Görüntüyü OpenCV formatına dönüştürme\n",
        "    frame = cv2.cvtColor(np.array(frame), cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # Video kaydediciye yazma\n",
        "    out.write(frame)\n",
        "\n",
        "out.release()\n",
        "cap.release()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86OikwhybMMc",
        "outputId": "80ddf0b5-dfbb-4eb1-b932-63ae0834606f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n",
            "non-fall\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Görüntüyü CUDA cihazına gönderme (eğer kullanılıyorsa)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "input_image = input_image.to(device)\n",
        "\n",
        "class_names = ['fall', 'non-fall']\n",
        "# Tahmin yapma\n",
        "output = model(input_image)\n",
        "_, predicted = torch.max(output, 1)\n",
        "\n",
        "print(\"Tahmin Sonucu:\")\n",
        "print(class_names[predicted.item()])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUxcqBtQ7mzc",
        "outputId": "96ae0115-e987-4797-b7b1-d407398dee69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tahmin Sonucu:\n",
            "fall\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eohjgOIDVnDq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}